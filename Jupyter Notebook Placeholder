# This is a placeholder for the Python code that would be in a Jupyter Notebook.
# It would contain the full, runnable code for the experiment.

# Instructions for running the experiment:
# 1. Install necessary libraries:
#    !pip install tensorflow
#    !pip install torch torchvision torchaudio

# 2. Import libraries
import tensorflow as tf
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from time import time
import torch.nn as nn
import torch.nn.functional as F

# Define the CNN model architecture for TensorFlow
def create_tf_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model

# Define the CNN model architecture for PyTorch
class PyTorchModel(nn.Module):
    def __init__(self):
        super(PyTorchModel, self).__init__()
        # PyTorch layers for the CNN
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(1600, 128)  # Adjusted input size based on layer calculations
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Apply the convolutional layers and activation functions
        x = self.conv1(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        
        # Flatten the output for the fully connected layers
        # The flatten size needs to be dynamically calculated
        # based on the output of the last max-pooling layer.
        # For a 28x28 input, it becomes 4x4 with 64 channels, so 1600.
        x = torch.flatten(x, 1)
        
        # Apply the fully connected layers
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        
        # Apply log-softmax for the final output
        output = F.log_softmax(x, dim=1)
        return output

# TensorFlow Training Function
def train_tensorflow():
    start_time = time()
    # Load and preprocess data
    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
    x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255
    model = create_tf_model()
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])
    model.fit(x_train, y_train, epochs=10, batch_size=64)
    end_time = time()
    return end_time - start_time

# PyTorch Training Function
def train_pytorch():
    start_time = time()
    # Load and preprocess data
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=64)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = PyTorchModel().to(device)
    optimizer = torch.optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss()

    for epoch in range(10):
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
    end_time = time()
    return end_time - start_time

# Main execution (would be run in a notebook cell)
tf_time = train_tensorflow()
print(f"TensorFlow Training Time: {tf_time:.2f} seconds")
pytorch_time = train_pytorch()
print(f"PyTorch Training Time: {pytorch_time:.2f} seconds")
