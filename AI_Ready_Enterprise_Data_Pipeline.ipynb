{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMONaZGyQ7yS3y8RosyGygF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsThien/Projects/blob/main/AI_Ready_Enterprise_Data_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def ingest_data():\n",
        "    \"\"\"\n",
        "    Downloads the COVID-19 dataset from Kaggle (imdevskp/corona-virus-report)\n",
        "    and loads the 'covid_19_clean_complete.csv' file into a Pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        df (pd.DataFrame): Loaded dataset\n",
        "    \"\"\"\n",
        "    print(\"Downloading dataset from Kaggle...\")\n",
        "    path = kagglehub.dataset_download(\"imdevskp/corona-virus-report\")\n",
        "\n",
        "    # Find the main data file\n",
        "    data_file = os.path.join(path, \"covid_19_clean_complete.csv\")\n",
        "\n",
        "    if not os.path.exists(data_file):\n",
        "        raise FileNotFoundError(f\"Expected file 'covid_19_clean_complete.csv' not found in {path}\")\n",
        "\n",
        "    # Load the dataset\n",
        "    print(\"Loading dataset into Pandas...\")\n",
        "    df = pd.read_csv(data_file)\n",
        "\n",
        "    # Preview schema\n",
        "    print(\"\\nDataset successfully loaded!\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(\"\\nColumns:\", list(df.columns))\n",
        "    print(\"\\nPreview:\")\n",
        "    print(df.head(5))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    df = ingest_data()\n"
      ],
      "metadata": {
        "id": "JGihwKV03eIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans the COVID-19 dataset:\n",
        "    - Converts Date to datetime\n",
        "    - Fills missing numeric values with 0\n",
        "    - Fills missing region or country names\n",
        "    - Removes duplicates\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Convert date column\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    # Fill missing text fields\n",
        "    for col in ['Province/State', 'Country/Region']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "    # Fill missing numeric columns\n",
        "    num_cols = ['Confirmed', 'Deaths', 'Recovered']\n",
        "    for col in num_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    print(\"Data cleaned successfully.\")\n",
        "    print(f\"Remaining missing values:\\n{df.isnull().sum().sort_values(ascending=False).head()}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds useful derived features:\n",
        "    1. ActiveCases = Confirmed - Deaths - Recovered\n",
        "    2. MortalityRate = Deaths / Confirmed (as %)\n",
        "    3. RecoveryRate = Recovered / Confirmed (as %)\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Derived feature 1: Active cases\n",
        "    df['ActiveCases'] = df['Confirmed'] - df['Deaths'] - df['Recovered']\n",
        "\n",
        "    # Derived feature 2: Mortality rate (%)\n",
        "    df['MortalityRate'] = np.where(df['Confirmed'] > 0, (df['Deaths'] / df['Confirmed']) * 100, 0)\n",
        "\n",
        "    # Derived feature 3: Recovery rate (%)\n",
        "    df['RecoveryRate'] = np.where(df['Confirmed'] > 0, (df['Recovered'] / df['Confirmed']) * 100, 0)\n",
        "\n",
        "    print(\"Feature engineering completed.\")\n",
        "    print(df[['Date', 'Country/Region', 'Confirmed', 'ActiveCases', 'MortalityRate', 'RecoveryRate']].head())\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "fiyRJo_S3hZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    df_raw = ingest_data()\n",
        "    df_clean = clean_data(df_raw)\n",
        "    df_final = feature_engineering(df_clean)\n",
        "\n",
        "    print(\"\\nTransformed dataset preview:\")\n",
        "    print(df_final.head(3))\n"
      ],
      "metadata": {
        "id": "-AdwteFv3jMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# LOGGING SETUP\n",
        "# -----------------------------------------------------------------------------\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "log_file = f\"logs/pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=log_file,\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "console.setFormatter(formatter)\n",
        "logging.getLogger().addHandler(console)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DATA INGESTION\n",
        "# -----------------------------------------------------------------------------\n",
        "def ingest_data():\n",
        "    \"\"\"Download the COVID-19 dataset and load it into a DataFrame.\"\"\"\n",
        "    logging.info(\"Starting data ingestion...\")\n",
        "\n",
        "    path = kagglehub.dataset_download(\"imdevskp/corona-virus-report\")\n",
        "\n",
        "    raw_dir = \"data/raw\"\n",
        "    os.makedirs(raw_dir, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(path, \"covid_19_clean_complete.csv\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logging.error(\"File not found in Kaggle dataset folder.\")\n",
        "        raise FileNotFoundError(\"covid_19_clean_complete.csv missing.\")\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.to_csv(os.path.join(raw_dir, \"covid_raw.csv\"), index=False)\n",
        "    logging.info(f\"Raw data saved to {raw_dir}/covid_raw.csv\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DATA CLEANING\n",
        "# -----------------------------------------------------------------------------\n",
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean the dataset and handle missing values.\"\"\"\n",
        "    logging.info(\"Cleaning data...\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    for col in ['Province/State', 'Country/Region']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "    for col in ['Confirmed', 'Deaths', 'Recovered']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    logging.info(f\"Cleaned data shape: {df.shape}\")\n",
        "    logging.info(f\"Missing values summary:\\n{df.isnull().sum().to_dict()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# FEATURE ENGINEERING\n",
        "# -----------------------------------------------------------------------------\n",
        "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add derived features for analysis.\"\"\"\n",
        "    logging.info(\"Performing feature engineering...\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df['ActiveCases'] = df['Confirmed'] - df['Deaths'] - df['Recovered']\n",
        "    df['MortalityRate'] = np.where(df['Confirmed'] > 0, (df['Deaths'] / df['Confirmed']) * 100, 0)\n",
        "    df['RecoveryRate'] = np.where(df['Confirmed'] > 0, (df['Recovered'] / df['Confirmed']) * 100, 0)\n",
        "\n",
        "    logging.info(\"Feature engineering complete. Added: ActiveCases, MortalityRate, RecoveryRate.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# STORAGE\n",
        "# -----------------------------------------------------------------------------\n",
        "def save_data(df: pd.DataFrame, stage: str):\n",
        "    \"\"\"Save DataFrame to Parquet and CSV in appropriate folder.\"\"\"\n",
        "    stage_dir = f\"data/{stage}\"\n",
        "    os.makedirs(stage_dir, exist_ok=True)\n",
        "\n",
        "    csv_path = os.path.join(stage_dir, f\"covid_{stage}.csv\")\n",
        "    parquet_path = os.path.join(stage_dir, f\"covid_{stage}.parquet\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_parquet(parquet_path, index=False)\n",
        "\n",
        "    logging.info(f\"Data saved to {csv_path} and {parquet_path}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# MAIN PIPELINE\n",
        "# -----------------------------------------------------------------------------\n",
        "def run_pipeline():\n",
        "    logging.info(\"Pipeline started.\")\n",
        "    try:\n",
        "        # Ingest\n",
        "        df_raw = ingest_data()\n",
        "        save_data(df_raw, \"raw\")\n",
        "\n",
        "        # Clean\n",
        "        df_clean = clean_data(df_raw)\n",
        "        save_data(df_clean, \"processed\")\n",
        "\n",
        "        # Transform\n",
        "        df_final = feature_engineering(df_clean)\n",
        "        save_data(df_final, \"output\")\n",
        "\n",
        "        logging.info(\"Pipeline completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Pipeline failed: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()\n"
      ],
      "metadata": {
        "id": "mOd01_KY32gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install prefect\n"
      ],
      "metadata": {
        "id": "XeQgrjQy36FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import logging\n",
        "from prefect import task, flow\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# LOGGING SETUP\n",
        "# -----------------------------------------------------------------------------\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "log_file = f\"logs/prefect_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=log_file,\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.INFO)\n",
        "logging.getLogger().addHandler(console)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PREFECT TASKS\n",
        "# -----------------------------------------------------------------------------\n",
        "@task(name=\"Ingest Data\")\n",
        "def ingest_data():\n",
        "    logging.info(\"Starting data ingestion...\")\n",
        "    path = kagglehub.dataset_download(\"imdevskp/corona-virus-report\")\n",
        "    data_file = os.path.join(path, \"covid_19_clean_complete.csv\")\n",
        "\n",
        "    df = pd.read_csv(data_file)\n",
        "    logging.info(f\"Data ingested: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "@task(name=\"Clean Data\")\n",
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logging.info(\"Cleaning data...\")\n",
        "    df = df.copy()\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    for col in ['Province/State', 'Country/Region']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "    for col in ['Confirmed', 'Deaths', 'Recovered']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    logging.info(f\"Cleaned data shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "@task(name=\"Feature Engineering\")\n",
        "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logging.info(\"Performing feature engineering...\")\n",
        "    df = df.copy()\n",
        "    df['ActiveCases'] = df['Confirmed'] - df['Deaths'] - df['Recovered']\n",
        "    df['MortalityRate'] = np.where(df['Confirmed'] > 0, (df['Deaths'] / df['Confirmed']) * 100, 0)\n",
        "    df['RecoveryRate'] = np.where(df['Confirmed'] > 0, (df['Recovered'] / df['Confirmed']) * 100, 0)\n",
        "    logging.info(\"Feature engineering complete.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "@task(name=\"Save Output\")\n",
        "def save_output(df: pd.DataFrame):\n",
        "    os.makedirs(\"data/output\", exist_ok=True)\n",
        "    out_path = f\"data/output/covid_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet\"\n",
        "    df.to_parquet(out_path, index=False)\n",
        "    logging.info(f\"Saved processed data to {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PREFECT FLOW (DAG)\n",
        "# -----------------------------------------------------------------------------\n",
        "@flow(name=\"COVID Data Pipeline\")\n",
        "def covid_pipeline():\n",
        "    \"\"\"Define the Prefect data pipeline DAG.\"\"\"\n",
        "    df_raw = ingest_data()\n",
        "    df_clean = clean_data(df_raw)\n",
        "    df_transformed = feature_engineering(df_clean)\n",
        "    save_output(df_transformed)\n",
        "    logging.info(\"Prefect pipeline run completed successfully.\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ENTRY POINT\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    covid_pipeline()\n"
      ],
      "metadata": {
        "id": "Ou42zQQo4Bod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Load transformed dataset (output from your Prefect pipeline)\n",
        "# ---------------------------------------------------------------------\n",
        "output_dir = \"data/output\"\n",
        "parquet_files = glob.glob(os.path.join(output_dir, \"*.parquet\"))\n",
        "\n",
        "if not parquet_files:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {output_dir}\")\n",
        "\n",
        "# Find the most recent file\n",
        "latest_file = max(parquet_files, key=os.path.getctime)\n",
        "\n",
        "print(f\"Loading data from: {latest_file}\")\n",
        "df = pd.read_parquet(latest_file)\n",
        "\n",
        "# Select numeric features\n",
        "features = ['ActiveCases', 'Deaths', 'Recovered']\n",
        "target = 'Confirmed'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Split data into training/testing\n",
        "# ---------------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Train a simple linear regression model\n",
        "# ---------------------------------------------------------------------\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Evaluate performance\n",
        "# ---------------------------------------------------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Example prediction\n",
        "sample = X_test.iloc[0:1]\n",
        "predicted_confirmed = model.predict(sample)\n",
        "print(f\"\\nPredicted Confirmed Cases for sample:\\n{sample}\\n→ {predicted_confirmed[0]:.0f}\")"
      ],
      "metadata": {
        "id": "Jm-voLA85gjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10af5741"
      },
      "source": [
        "yaml_config = \"\"\"\n",
        "# Configuration for COVID-19 Data Pipeline\n",
        "\n",
        "dataset:\n",
        "  kaggle_id: \"imdevskp/corona-virus-report\"\n",
        "  main_file: \"covid_19_clean_complete.csv\"\n",
        "\n",
        "paths:\n",
        "  raw_dir: \"data/raw\"\n",
        "  processed_dir: \"data/processed\"\n",
        "  output_dir: \"data/output\"\n",
        "  logs_dir: \"logs\"\n",
        "\n",
        "storage:\n",
        "  output_format: \"parquet\"\n",
        "\n",
        "pipeline:\n",
        "  test_size: 0.2\n",
        "  random_state: 42\n",
        "\"\"\"\n",
        "\n",
        "with open(\"config.yaml\", \"w\") as f:\n",
        "    f.write(yaml_config)\n",
        "\n",
        "print(\"config.yaml created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "COVID-19 Data Pipeline\n",
        "----------------------\n",
        "\n",
        "A modular data pipeline that ingests, cleans, transforms, and saves COVID-19 data\n",
        "from Kaggle using configurable parameters defined in `config.yaml`.\n",
        "\n",
        "This pipeline follows enterprise-grade best practices:\n",
        "- Configuration management (YAML)\n",
        "- Logging\n",
        "- Reproducible directory structure\n",
        "- PEP8-compliant, modular code\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Load Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "def load_config(path=\"config.yaml\"):\n",
        "    \"\"\"Load pipeline configuration from YAML file.\"\"\"\n",
        "    with open(path, \"r\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Logging Setup\n",
        "# -----------------------------------------------------------------------------\n",
        "def setup_logging(log_dir: str):\n",
        "    \"\"\"Initialize logging system.\"\"\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    log_file = os.path.join(log_dir, f\"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "\n",
        "    logging.basicConfig(\n",
        "        filename=log_file,\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    )\n",
        "    console = logging.StreamHandler()\n",
        "    console.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "    console.setFormatter(formatter)\n",
        "    logging.getLogger().addHandler(console)\n",
        "    return log_file\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data Ingestion\n",
        "# -----------------------------------------------------------------------------\n",
        "def ingest_data(dataset_id: str, filename: str, raw_dir: str) -> pd.DataFrame:\n",
        "    \"\"\"Download dataset from Kaggle and load it into a DataFrame.\"\"\"\n",
        "    logging.info(\"Starting data ingestion...\")\n",
        "    path = kagglehub.dataset_download(dataset_id)\n",
        "    file_path = os.path.join(path, filename)\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    os.makedirs(raw_dir, exist_ok=True)\n",
        "    raw_path = os.path.join(raw_dir, \"covid_raw.csv\")\n",
        "    df.to_csv(raw_path, index=False)\n",
        "\n",
        "    logging.info(f\"Raw data saved to {raw_path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data Cleaning\n",
        "# -----------------------------------------------------------------------------\n",
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean dataset: handle missing values and data types.\"\"\"\n",
        "    logging.info(\"Cleaning data...\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    if \"Date\" in df.columns:\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "    for col in [\"Province/State\", \"Country/Region\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "    for col in [\"Confirmed\", \"Deaths\", \"Recovered\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    logging.info(f\"Cleaned data shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Feature Engineering\n",
        "# -----------------------------------------------------------------------------\n",
        "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add derived metrics useful for analysis and AI modeling.\"\"\"\n",
        "    logging.info(\"Performing feature engineering...\")\n",
        "    df = df.copy()\n",
        "\n",
        "    df[\"ActiveCases\"] = df[\"Confirmed\"] - df[\"Deaths\"] - df[\"Recovered\"]\n",
        "    df[\"MortalityRate\"] = np.where(df[\"Confirmed\"] > 0,\n",
        "                                   (df[\"Deaths\"] / df[\"Confirmed\"]) * 100, 0)\n",
        "    df[\"RecoveryRate\"] = np.where(df[\"Confirmed\"] > 0,\n",
        "                                  (df[\"Recovered\"] / df[\"Confirmed\"]) * 100, 0)\n",
        "\n",
        "    logging.info(\"Feature engineering complete.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Save Data\n",
        "# -----------------------------------------------------------------------------\n",
        "def save_data(df: pd.DataFrame, output_dir: str, output_format: str = \"parquet\"):\n",
        "    \"\"\"Save processed data to specified folder and format.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_path = os.path.join(output_dir, f\"covid_output_{timestamp}.{output_format}\")\n",
        "\n",
        "    if output_format == \"parquet\":\n",
        "        df.to_parquet(output_path, index=False)\n",
        "    else:\n",
        "        df.to_csv(output_path, index=False)\n",
        "\n",
        "    logging.info(f\"Data saved to {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Pipeline\n",
        "# -----------------------------------------------------------------------------\n",
        "def run_pipeline(config_path=\"config.yaml\"):\n",
        "    \"\"\"Execute full ETL pipeline.\"\"\"\n",
        "    config = load_config(config_path)\n",
        "    setup_logging(config[\"paths\"][\"logs_dir\"])\n",
        "\n",
        "    logging.info(\"Starting COVID-19 data pipeline.\")\n",
        "\n",
        "    df_raw = ingest_data(config[\"dataset\"][\"kaggle_id\"],\n",
        "                         config[\"dataset\"][\"main_file\"],\n",
        "                         config[\"paths\"][\"raw_dir\"])\n",
        "\n",
        "    df_clean = clean_data(df_raw)\n",
        "    df_final = feature_engineering(df_clean)\n",
        "\n",
        "    save_data(df_final,\n",
        "              config[\"paths\"][\"output_dir\"],\n",
        "              config[\"storage\"][\"output_format\"])\n",
        "\n",
        "    logging.info(\"Pipeline completed successfully.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()\n"
      ],
      "metadata": {
        "id": "jnNLPoGB6n-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prefect --quiet\n",
        "!pip install prefect graphviz --quiet"
      ],
      "metadata": {
        "id": "LRPCNaDaBg0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prefect import flow, task\n",
        "\n",
        "@task\n",
        "def ingest_data():\n",
        "    print(\"Ingesting data...\")\n",
        "    return {\"raw_data\": [1, 2, 3, 4, 5]}\n",
        "\n",
        "@task\n",
        "def clean_data(data):\n",
        "    print(\"Cleaning data...\")\n",
        "    cleaned = [x for x in data[\"raw_data\"] if x > 2]\n",
        "    return {\"cleaned_data\": cleaned}\n",
        "\n",
        "@task\n",
        "def feature_engineering(data):\n",
        "    print(\"Feature engineering...\")\n",
        "    features = [x**2 for x in data[\"cleaned_data\"]]\n",
        "    return {\"features\": features}\n",
        "\n",
        "@task\n",
        "def save_output(data):\n",
        "    print(\"Saving output...\")\n",
        "    print(\"Final features:\", data[\"features\"])\n",
        "    return True\n",
        "\n",
        "@flow\n",
        "def etl_flow():\n",
        "    raw = ingest_data()\n",
        "    cleaned = clean_data(raw)\n",
        "    feats = feature_engineering(cleaned)\n",
        "    save_output(feats)\n"
      ],
      "metadata": {
        "id": "_qvZUKUDBh5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def visualize_etl_dag():\n",
        "    dot = Digraph(comment=\"ETL Workflow (Prefect)\", format='png')\n",
        "    dot.attr(rankdir='LR', size='6,3')\n",
        "\n",
        "    # Define nodes\n",
        "    dot.node(\"A\", \"ingest_data\")\n",
        "    dot.node(\"B\", \"clean_data\")\n",
        "    dot.node(\"C\", \"feature_engineering\")\n",
        "    dot.node(\"D\", \"save_output\")\n",
        "\n",
        "    # Define edges\n",
        "    dot.edges([\"AB\", \"BC\", \"CD\"])\n",
        "\n",
        "    # Render and display\n",
        "    dot.render(\"etl_dag\", cleanup=True)\n",
        "    display(Image(\"etl_dag.png\"))\n",
        "\n",
        "visualize_etl_dag()\n"
      ],
      "metadata": {
        "id": "eTMAlZkYCXzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc33cf0a"
      },
      "source": [
        "## Enterprise-Grade COVID-19 Data Pipeline Summary\n",
        "\n",
        "This data pipeline demonstrates several key characteristics of an enterprise-grade solution, focusing on robustness, maintainability, and scalability.\n",
        "\n",
        "**What makes this pipeline \"enterprise-grade\":**\n",
        "\n",
        "*   **Modularity:** The pipeline is broken down into distinct functions for ingestion, cleaning, feature engineering, and saving data. This modularity makes the code easier to understand, test, and maintain.\n",
        "*   **Configuration Management:** Using a `config.yaml` file externalizes parameters like dataset IDs, file paths, and processing options. This allows for easy modification of pipeline behavior without changing the code, promoting flexibility and reusability.\n",
        "*   **Logging:** The pipeline incorporates comprehensive logging at various stages. This provides visibility into the pipeline's execution, making it easier to debug issues, monitor performance, and track data flow.\n",
        "*   **Reproducible Directory Structure:** The pipeline saves data to organized directories (`data/raw`, `data/processed`, `data/output`) based on the processing stage. This standardized structure improves data governance and makes it clear where to find data at different points in the pipeline.\n",
        "*   **Error Handling:** While not explicitly shown in the Prefect flow definition, the underlying functions include basic error handling (e.g., `FileNotFoundError` in `ingest_data`). In a full enterprise system, more comprehensive error handling and alerting would be implemented.\n",
        "*   **Use of Libraries:** The pipeline leverages established libraries like pandas, numpy, kagglehub, and Prefect, which are industry standards for data manipulation, access, and workflow orchestration.\n",
        "\n",
        "**Which best practices were applied:**\n",
        "\n",
        "*   **Separation of Concerns:** Each function focuses on a single task (ingest, clean, transform, save), adhering to the principle of separation of concerns.\n",
        "*   **Parameterization:** Using the `config.yaml` file allows for parameterization of the pipeline, making it adaptable to different datasets or configurations without code changes.\n",
        "*   **Code Readability and Documentation:** The code includes docstrings explaining the purpose of each function, improving readability and maintainability.\n",
        "*   **Use of Standard File Formats:** Saving data in Parquet format (a columnar storage format) is a best practice for large datasets, offering improved performance for analytical queries.\n",
        "\n",
        "**How orchestration (using Prefect) improves maintainability:**\n",
        "\n",
        "Orchestration tools like Prefect provide a framework for defining, scheduling, and monitoring data pipelines as directed acyclic graphs (DAGs). This offers several benefits for maintainability:\n",
        "\n",
        "*   **Visibility and Monitoring:** Prefect provides a UI that visualizes the pipeline's execution, allowing developers to easily see the status of each task, identify bottlenecks, and diagnose failures.\n",
        "*   **Error Handling and Retries:** Prefect can automatically handle task failures by retrying them, improving the pipeline's resilience.\n",
        "*   **Scheduling:** Flows can be scheduled to run automatically at predefined intervals, eliminating the need for manual execution and ensuring data is processed regularly.\n",
        "*   **Dependency Management:** Prefect explicitly defines the dependencies between tasks, ensuring that tasks are executed in the correct order. This makes the pipeline's logic clear and prevents issues caused by incorrect task sequencing.\n",
        "*   **Modularity and Reusability:** Tasks defined in Prefect can be reused across different flows, reducing code duplication and improving maintainability.\n",
        "\n",
        "By applying these practices and leveraging orchestration, the pipeline becomes more robust, easier to manage, and scalable for handling larger datasets and more complex workflows in an enterprise environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "etl_flow()"
      ],
      "metadata": {
        "id": "jwX0c7t8CkOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}